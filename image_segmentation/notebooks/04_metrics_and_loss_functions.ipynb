{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter - Metrics and Loss functions \n",
    "\n",
    "- Metrics \n",
    "- Loss functions \n",
    "\n",
    "Before discussion these both concepts. lets call our validation data to and process using the model we have saved in the earlier section. We will use these predictions and original labels of validation data to calculate all the metrics and dicuss the loss functions further.\n",
    "\n",
    "Note: Since our **agumentation** function in the coco_dataloader.py file has randomness involved in it (choosing random scales and choosing random locations for crop), I have created another function called **val_augmentation**. This uses only one scale and removes random cropping and flipping, which helps in replicating the results.\n",
    "\n",
    "```python\n",
    "from coco_dataloader import aspect_ratio_calc, resize_image, pad_image\n",
    "\n",
    "def val_augmentation(image, label, resize, crop_size, mean_bgr, ignore_label=255):\n",
    "    \n",
    "    ## h and w select with changing aspect ratio\n",
    "    h, w = aspect_ratio_calc(image, label, resize)\n",
    "    \n",
    "    ## resize\n",
    "    image, label = resize_image(image, label, (int(h), int(w)))\n",
    "    \n",
    "    # Padding to fit for crop_size\n",
    "    image, label = pad_image(image, label, crop_size, mean_bgr, ignore_label)\n",
    "    \n",
    "    ## crop the necessary portion of the image.\n",
    "    image = image[:crop_size, :crop_size]\n",
    "    label = label[:crop_size, :crop_size]\n",
    "    return image, label\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/nfshome1/FRACTAL/vanapalli.prakash/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfshome1/FRACTAL/vanapalli.prakash/miniconda3/envs/keras/lib/python3.6/site-packages/classification_models/resnext/__init__.py:4: UserWarning: Current ResNext models are deprecated, use keras.applications ResNeXt models\n",
      "  warnings.warn('Current ResNext models are deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded\n"
     ]
    }
   ],
   "source": [
    "## import necessary libraries \n",
    "from segmentation_models.backbones import get_preprocessing\n",
    "from segmentation_models import Unet\n",
    "import numpy as np \n",
    "import glob\n",
    "\n",
    "## Calling val ids \n",
    "val_ids = [i.rsplit(\"/\")[-1].rsplit(\".\")[0] for  i in glob.glob(\"../../data/cocostuff/images/val/*.jpg\")]\n",
    "\n",
    "## Load the model  and the preprocessing func\n",
    "BACKBONE = 'resnet50'\n",
    "preprocess_input = get_preprocessing(BACKBONE)\n",
    "model = Unet(BACKBONE, encoder_weights='imagenet', classes=1, activation=\"sigmoid\")\n",
    "\n",
    "## Load the trained weights \n",
    "model.load_weights(\"../../data/cocostuff/model.h5\")\n",
    "print(\"Model weights loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [00:36<00:00, 15.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564 564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from coco_dataloader import get_image_and_mask, val_augmentation\n",
    "from tqdm import tqdm \n",
    "\n",
    "root=\"../../data/cocostuff/images/\"\n",
    "folder_name = \"val\"\n",
    "\n",
    "original = []\n",
    "predicted = []\n",
    "for i in tqdm(val_ids):\n",
    "    ## get image and mask\n",
    "    image, label = get_image_and_mask(root, folder_name, i)\n",
    "    \n",
    "    ## preprocess using val augmentation\n",
    "    image, label = val_augmentation(image, label, 512, 448, (0, 0, 0), 0)\n",
    "    \n",
    "    ## preprocess as per network requirement\n",
    "    image_preprocess = preprocess_input(image)\n",
    "    \n",
    "    ## convert to float\n",
    "    image_final = np.expand_dims(image_preprocess, 0).astype(np.float64)\n",
    "    \n",
    "    ## predict on image\n",
    "    pred = model.predict(image_final)\n",
    "    \n",
    "    ## Store the labels\n",
    "    original.append(label)\n",
    "    predicted.append(pred)\n",
    "print(len(original), len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564, 448, 448)\n"
     ]
    }
   ],
   "source": [
    "## Concatenating all the predictions \n",
    "final_preds = np.concatenate([np.squeeze(i, 3) for i in predicted])\n",
    "final_preds = np.where(final_preds>0.5, 1, 0) ## Using a threshold of 0.5 \n",
    "print(final_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564, 448, 448)\n"
     ]
    }
   ],
   "source": [
    "## Concatenating all the original labels \n",
    "final_orig = np.concatenate([np.expand_dims(i, 0) for i in original])\n",
    "print(final_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## b is the total number of images \n",
    "## h is height \n",
    "## w is width\n",
    "b, h, w = final_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Metrics \n",
    "Metrics are the way of knowing how well your model is performing. In the above examples we have used iou_score without much knowledge about it.  In segmentation there are various metrics used to calculate the network performace. Lets look at a few of the most important metrics below and the use cases when we need to use it.  \n",
    "1) f1_score or dice_score  \n",
    "2) f2_score  \n",
    "3) iou_score  \n",
    "4) pixel accuracy.\n",
    "\n",
    "Before going in depth of metrics, lets dive into some of the terminology. The below diagram from wikipedia comes handy to answer a lot of questions.\n",
    "\n",
    "![confustion_matrix](../images/confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362139917695603"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 2\n",
    "smooth = 1e-12\n",
    "tp  = 0.91\n",
    "fp = 0.19\n",
    "fn = 0.03 \n",
    "\n",
    "score = ((1 + beta ** 2) * tp + smooth) \\\n",
    "/ ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856757979642156"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(final_orig == final_preds).sum()/ (564 * 448 * 448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision \n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Here in our case, precision means of the predicted human pixels, how many are actually human pixels.\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision = tp/(tp+fp)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7980823777051048"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Of all the original pos labels. How many are actually predicted as pos ?\n",
    "tp = np.sum(final_preds[np.where(final_orig == 1)])\n",
    "\n",
    "## of all the original neg labels, how many are actually predicted as pos ?\n",
    "fp = np.sum(final_preds[np.where(final_orig == 0)])\n",
    "\n",
    "## precision is defined above\n",
    "precision = tp/(tp+fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall (Sensitivity)\n",
    "Recall is the ratio of correctly predicted positive observations to the all positive observations in actual class. In our use case, recall means of all the original human pixels, how many are correctly predicted as human pixels.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall = tp/(tp+fn)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9136739722577483\n"
     ]
    }
   ],
   "source": [
    "## Of all the original pos labels. How many are actually predicted as pos ?\n",
    "tp = np.sum(final_preds[np.where(final_orig == 1)])\n",
    "\n",
    "## of all the original pos labels, how many are actually predicted as neg ?\n",
    "fn = final_preds[np.where(final_orig == 1)].shape[0] - np.sum(final_preds[np.where(final_orig == 1)])\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel Accuracy\n",
    "Accuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. \n",
    "\n",
    "\\begin{equation*}\n",
    "Accuracy = (tp+ tn)/(tp+fp+fn+tn)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856757979642156\n"
     ]
    }
   ],
   "source": [
    "## Of all the original pos labels. How many are actually predicted as pos ?\n",
    "tp = np.sum(final_preds[np.where(final_orig == 1)])\n",
    "\n",
    "## of all the original neg labels, how many are actually predicted as pos ?\n",
    "fp = np.sum(final_preds[np.where(final_orig == 0)])\n",
    "\n",
    "## of all the original pos labels, how many are actually predicted as neg ?\n",
    "fn = final_preds[np.where(final_orig == 1)].shape[0] - np.sum(final_preds[np.where(final_orig == 1)])\n",
    "\n",
    "\n",
    "## of all the original neg labels, how many are actually predicted as neg?\n",
    "tn = final_preds[np.where(final_orig == 0)].shape[0] - np.sum(final_preds[np.where(final_orig == 0)])\n",
    "\n",
    "\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 score or Dice Score \n",
    "F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision) \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "which can also be written as  \n",
    "\n",
    "\\begin{equation*}\n",
    "F1Score = 2TP/ (2TP + FP + FN)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851975336609741\n"
     ]
    }
   ],
   "source": [
    "f1_score = (2*tp)/(2*tp+ fp+ fn)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2 Score \n",
    "Since F1 score is only the hormonic mean of precision and recall it weights precision and recall the same way. What if you want to keep more weight to precision then recall or the reverse, here comes the **fbeta_score**. \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "f_\\beta score = ((1 + \\beta^{2}) * tp ) \\\n",
    "/ ((1 + \\beta^{2}) * tp + \\beta^{2} * fn + fp )\n",
    "\\end{equation*}\n",
    "\n",
    "The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0. The beta parameter determines the weight of precision in the combined score. beta < 1 lends more weight to precision, while beta > 1 favors recall (beta -> 0 considers only precision, beta -> inf only recall).\n",
    "\n",
    "F2 score means keeping beta value at 2, which means favoring recall more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8879523595533807\n"
     ]
    }
   ],
   "source": [
    "beta = 2 \n",
    "fbeta_score = ((1 + beta ** 2) * tp ) \\\n",
    "/ ((1 + beta ** 2) * tp + beta ** 2 * fn + fp )\n",
    "print(fbeta_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOU score \n",
    "The Jaccard Index also known as Intersection over Union and the jaccard similarity coefficient is a statistic used to compare the similarity and diversity of sample sets. The jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection by the size of the union of the sample sets. \n",
    "\n",
    "In simple terms we can say that it is the ratio of area of overlap to the area of union. We can understand this using a venn diagram below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![IOU score](../images/iou.png)\n",
    "\n",
    "In the above diagram, we can see there are two rectangles, The top rectangle is called ground truth (In our case these are actual human pixels), the bottom rectangle is the predicted one (Algo predicted human pixels). In an ideal case both these rectangle should have full over lap, but sometimes we miss predicting some object (human) pixels (False negatives - Represented in yellow) and sometimes we predict non-object pixels (non-human pixels) as object (human) pixels (False negatives- Represented in red). So IoU as metric takes care of both these things. It is the area of correctly predicted pixels (true positives) to the total area of (true positives, false positives and false negatives). In terms of equation, we can write as follows.\n",
    "\n",
    "In terms of TP, FP and FN it can be written as \n",
    "\\begin{equation*}\n",
    "IoU = TP/(TP+FP+FN)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- For multi-class classification problems, IoU is calculated for each object, averaged across each class and then average over all the classes.\n",
    "- The maximum value is 1 when FP and FN are zero. The minimum value is zero when TP is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7421228513451552"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_score = (tp)/(tp+fp+fn)\n",
    "iou_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "Q) When both precision and recall are of equal importance, which of the following metric is best to use? (one or more correct answers).  \n",
    "A) F1 Score  \n",
    "B) F2 Score  \n",
    "C) pixel accuracy  \n",
    "D) Any of the metric is okay.  \n",
    "\n",
    "Ans) A, D. F1 score is used when precision and recall are of equal importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "In the above example what is F0.5 score ?\n",
    "\n",
    "(a) the objective of performing the task i.e. what will the learner perform and gain out of that task, \n",
    "To calculate the F0.5 score.\n",
    "\n",
    "(b) a set of instructions and \n",
    "- replace beta value with 0.5 in the F2_score section\n",
    "\n",
    "(c) the solution code. \n",
    "```python\n",
    "beta = 0.5\n",
    "fbeta_score = ((1 + beta ** 2) * tp ) \\\n",
    "/ ((1 + beta ** 2) * tp + beta ** 2 * fn + fp )\n",
    "print(fbeta_score)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions \n",
    "For any deep learning models we need to have a loss function that needs to be optimized. In the above experiements we have used **bce_jaccard_loss** without much knowledge about it. In this section lets look at the intution behind each loss function and how they are calculated.\n",
    "\n",
    "The major loss functions include\n",
    "- Jaccard loss\n",
    "- dice_loss \n",
    "\n",
    "\n",
    "## Jaccard loss or BCE_Jaccard loss or cce_jaccard loss\n",
    "- Jaccard loss is 1-iou_score. \n",
    "- Cross entropy loss evaluates the class predictions for each pixel vector individually and then averages over all pixels, we're essentially asserting equal learning to each pixel in the image. \n",
    "\n",
    "\\begin{equation*}\n",
    "CE = \\sum_{Nclasses} y_{true} * log(y_{pred})\n",
    "\\end{equation*}\n",
    "\n",
    "- Since jaccard loss is essentially the measure of overlap between two samples and cross entropy loss evaluates at pixel level. Combaining both these loss functions will help the network train better. This is empherically tested and is generally phenomena across data science practioners. \n",
    "- In multi-class classification problem, jaccard loss is calculated first individually for each class as mentioned above and later averaged over all the classes. for multi-classification loss we categorical cross entropy loss instead of binary cross entropy loss.\n",
    "\n",
    "\n",
    "## Dice loss or bce_dice_loss or cce_dice_loss \n",
    "- Dice_loss is 1-f2_score.\n",
    "- It is very similar to what we have discussed previously except that instead of iou_score we use f2_score.\n",
    "- For multi-class classification too dice loss is calculated for each class separately and later averaged over all the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Notes:\n",
    "- In the first section, We have seen the fundamental difference between image classification and segmentation, The practical use cases of the segmentation across different industries. We have later seen the different kinds of segmentation along with uses cases for each one.\n",
    "- In the second section, we have seen the public datasets available for semantic segmentation and how to process cocostuff dataset using cocostuffapi.\n",
    "- In the third section, we have seen how to construct a data loader for semantic segmentation task using keras.utils.sequence module and also learnt how to do different kinds of augmentation and color transformation\n",
    "- In the fourth section we have trained segmentation model using unet architecutures, introduced segmentation models repo and discussed various architecutures available.\n",
    "- In the final section, we have looked into various metrics and loss functions used in semantic segmentation.\n",
    "\n",
    "With this, we have come to an end. Now you are equipped with all the tools necessary to deal with semantic segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
