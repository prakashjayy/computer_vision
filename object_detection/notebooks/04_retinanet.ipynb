{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter- working of RetinaNet\n",
    "\n",
    "- Topic-1 Introduction to RetinaNet\n",
    "- Topic-2 Network Architecuture design\n",
    "- Topic-3 Anchor box design\n",
    "- Topic-4 Encoding Ground truth box \n",
    "- Topic-5 Calculating loss functions \n",
    "\n",
    "## Topic-1 Introduction to RetinaNet\n",
    "RetinaNet is a one stage object detection framework prposed by Kaiming He and his colleuges from FAIR (Facebook AI Research) in 2018. In order to undestand the complete workings of the Retinanet students need to understand the following two papers written by the feam \n",
    "- [Feature Pyramid Networks for object detection](https://arxiv.org/pdf/1612.03144.pdf) - To solve localization of objects problem along with detecting large and small objects accurately. \n",
    "- [Focal Loss for Dense object detection](https://arxiv.org/pdf/1708.02002.pdf) - To solve the class imbalance problem discussed previously\n",
    "\n",
    "\n",
    "Some of the key features of the Retinanet network include. \n",
    "- Using ResNext-101 as backbone Retinanet reaches 40.8 mAP@(0.5:0.95) , 61.1 mAP@0.5 and 44.1 mAP@0.75. \n",
    "- It is faster and has considerably much better mAP compared two stage framework network.\n",
    "\n",
    "In this chapter, we will look into the deeper workings of RetinaNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-2 Basic Network Architecuture Design\n",
    "\n",
    "We have already seen an architecture before and learned how object detection is performed on it, lets reiterate that here once again. Suppose we have an image of size (800, 800) and we subsampled the image size to (50x50) using a neural network as shown in the below diagram. The subsampling ratio here is 16 (800/50). Then there are two brances, one branch predicts the location of the anchor box and the other head predicts the classification of box.\n",
    "- The backbone network here can be any image classification network like resnet, resnext, densenet, vgg etc. \n",
    "- We can see in the diagram that the output feature map has two parallel networks\n",
    "**Regression head**:  \n",
    "It is responsible for predicting the location of object wrt to the anchor box.  \n",
    "\n",
    "**Classifciation head**:  \n",
    "It is responsible for predicting the class of the object present the predicted box wrt to anchor box. There is no objectness score here because we use sigmoid. sigmoid tells wheather a particlar object is present or not. So instead of answering which object center is present in each anchor box, we answer weather a particular class (cow) is present or not in the image? when asking this question to all the objects. For example, suppose this is a 2 class object detector (cows and rabbit). Now if the threshold is 0.5 and the output probabilites after sigmoid (indenpendent classifiers) is [0.4, 0.7], we can say that rabbit is present in the predicted box with 0.7 confidence. Unlike softmax, these two terms doesn't sum to 1. If the output probablites is [0.1, 0.2], then the predicted anchor box doesn't contain any object. \n",
    "    \n",
    "![retinanet network](../images/retina_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with basic architecuture.\n",
    "Though convnets are robust to variance in scale, all the top entries in ImageNet or COCO challenges have used multi-scale testing on featurized image pyramids. For example, instead of sending only (800 x 800) image, say we send  (400 x 400) image, (1200 x 1200) image through the network. Now the feature map size for (400 x 400) image, 800 x 800 image and 1200 x 1200 image will be (50 x 50), (25 x 25) and (60 x 60) respectively. The researcher used this approach because most of the algorithms discussed above have miserable failed at detecting small objects and increasing/ decreasing the image size is one way of finding objects at different scale. As we have discussed in the first chapter,  This technique is nothing but image pyramids, though we have used only 3 scales here, we can use multiple scales. Image pyramids imporved the detection accuracy but the problem is that it increased the inference time.\n",
    "\n",
    "![image pyramids](../images/retinanet_02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Pyramids - Alternative effective solution for Image pyramids.\n",
    "If we carefully observe ResNet architecure the size of the feature map reduces by half after every resblock. The below diagram shows the size of feature map size of Resnet50 after every resblock. Now one obvious question we have is why are we only taking features from resblock5 (subsample 16) and not from all other resblocks? We will answer the reason in short time but in RetinaNet, with minor modifications to the network, the authors have used other resblocks too as shown in the below diagram.\n",
    "\n",
    "![Resnet](../images/retinanet_03.png)\n",
    "\n",
    "Previously we have only used stride 16 to determine the location. Now by using stride 8, 16, 32, 64, 128 we have feature maps of different sizes. The Intuition is that the feature maps of earlier layers will be responsible for detecting small objects, the feature maps in the later layers will be effective in finding large objects. One problem is that, due to multiple convolution layers, the later layers looses the spatial information of the object but have strong semantics whereas the earlier layers have strong spatial information but don't have strong object semantics.  As discussed in the paper [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf) In the earlier layers of the convolutional network, the model mostly learns edges and colors which will not help in detecting the objects exactly. In the later layers, the object semantics are very strong but due to multiple convolutions we loose the location information. The below diagram is extracted from  [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf) which clearly shows this phenomeno.  \n",
    "\n",
    "![Resnet](../images/retinanet_04.png)\n",
    "\n",
    "So The earlier feature map with size (100 x 100) though has good localization capability will not have power to identify the object. In the later layers, suppose the feature map size of (13 x 13) has strong object semantics but has poor localization capabilities (As multiple convolutions were applied). So in-order to solve this problem, the authors came up with an approach called Lateral connections using top-down pathway\n",
    "\n",
    "\n",
    "### Lateral Connections using top-down pathway.\n",
    "- Since lower layers have strong localization features and layers higher in the network contains strong semantic features combaining these two layers would lead to strong localization, strong semantic features. This combination is done using lateral connections as shown in the diagram.   \n",
    "\n",
    "![Resnet](../images/retinanet_05.png)\n",
    "\n",
    "- The Green block shown in the diagram is a Resnet architecture. C3, C4, C5 are the outputs of Resblock3, Resblock4 and Resblock5 respectively. These have filter map sizes of 100 x 100, 50 x 50, 25 x 25 with feature maps of 512, 1024, 2048 respectively.\n",
    "- C3, C4, C5 feature maps filters are reduced to 256 using 1 x 1 conv layer. Lets term these C3_reduced, C4_reduced, C5_reduced respectively. \n",
    "- A 3 x 3 conv is connected to C5_reduced which outputs **P5**. The P5 size of 25 x 25 x 256.\n",
    "- C5_reduced is upsampled from 25 x 25 to 50 x 50. This is then added to C4_reduced.  A 3 x 3 conv is applied to the added matrix which outputs **P4**. The P4 size is 50 x 50 x 256\n",
    "- C4_reduced is upsampled from 50 x 50 to 100 x 100. This is then added to C3_reduced.  A 3 x 3 conv is applied to the added matrix which outputs **P3**. The P4 size is 100 x 100 x 256\n",
    "- A 3 x 3 conv block is applied to Resblock5 output which gives an output size of 13 x 13. This is termed as **P6**.\n",
    "- A 3 x 3 conv block is applied to P6 output which gives an output size of 7 x 7. This is termed as **P7**\n",
    "- The FPN layer finally gives [P3, P4, P5, P6, P7] as outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetinaNet - Final Network\n",
    "There are two subnetworks called classification network and regression network which independently works on each of the FPN outputs P3, P4, P5, P6, P7. As shown in the diagram, the subnetworks consists of 4 3 x 3 conv layers where the feature map size is not altered (256 filter maps of P3 remains 256 filter maps at output).  The final 3x3 conv layer of conv classification network increases the number of filter maps to n_classes x n_anchors (In the diagram we have used n_classes = 80 and n_anchors = 9). The final 3x3 conv layer of regression layer increase the number of filter maps to 4 x n_anchors (Each box has x, y, w, h prediction so we have 4, n_anchors = 9). P3, P4, P5, P6, P7 outputs obtained from Classification and regression network are  reshaped and concatenated. The network now output 1200087 anchor box results, with classification network giving the output of class and regression network giving the output of location of the object.\n",
    "\n",
    "![RetinaNet final network](../images/retinanet_06.png)\n",
    "\n",
    "- below is the keras code to obtain the same\n",
    "- We can use the following backends \n",
    "\n",
    "\n",
    "### Keras_retinanet\n",
    "keras_retinanet is a github project by fizyr and team. As the name suggests it is a retinanet implementation in Keras. The [package](https://github.com/fizyr/keras-retinanet) has various backend networks. \n",
    "\n",
    "|Network name| Networks |\n",
    "|--------| ------- |\n",
    "| Densenet| densenet121, densenet169, densenet201 |\n",
    "| mobilenet|mobilenet128, mobilenet160, mobilenet192, mobilenet224 |\n",
    "| vgg | vgg16, vgg19|\n",
    "| resnet| resnet50, resnet101, resnet152|\n",
    "\n",
    "\n",
    "\n",
    "To install keras_retinanet in your system, use the following instructions.\n",
    "\n",
    "- Clone the repository in your system. \n",
    "```bash\n",
    "git clone https://github.com/fizyr/keras-retinanet\n",
    "```\n",
    "- Ensure numpy is installed using \n",
    "```bash\n",
    "pip install numpy --user\n",
    "```\n",
    "\n",
    "- In the repository, execute \n",
    "```bash\n",
    "pip install . --user. \n",
    "```\n",
    "Note that due to inconsistencies with how tensorflow should be installed, this package does not define a dependency on tensorflow as it will try to install that (which at least on Arch Linux results in an incorrect installation). Please make sure tensorflow is installed as per your systems requirements.\n",
    "\n",
    "\n",
    "Alternatively, you can run the code directly from the cloned repository, however you need to run \n",
    "```bash\n",
    "python setup.py build_ext --inplace\n",
    "```\n",
    "to compile Cython code first.\n",
    "Optionally, install pycocotools if you want to train / test on the MS COCO dataset by running \n",
    "```bash\n",
    "pip install --user git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_retinanet import models\n",
    "import keras \n",
    "\n",
    "backbone = models.backbone(\"resnet50\")\n",
    "inputs = keras.layers.Input(shape=(800, 800, 3))\n",
    "retinanet = backbone.retinanet(80, num_anchors=None, modifier=None, inputs=inputs)\n",
    "#print(retinanet.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "1) Total trainable params for densnet121 backend ?  \n",
    "A) 18, 872, 628  \n",
    "B) 18, 791, 028  (Ans)  \n",
    "C) 38, 021, 812   \n",
    "D) 37, 915, 572   \n",
    "\n",
    "```python \n",
    "from keras_retinanet import models\n",
    "import keras \n",
    "\n",
    "backbone = models.backbone(\"densenet121\")\n",
    "inputs = keras.layers.Input(shape=(800, 800, 3))\n",
    "retinanet = backbone.retinanet(80, num_anchors=None, modifier=None, inputs=inputs)\n",
    "print(retinanet.summary())\n",
    "```\n",
    "\n",
    "2) Which of the following statements are True about Retinanet?  \n",
    "A) RetinaNet uses top-down pathway lateral connections  \n",
    "B) RetinaNet uses bottom-up pathway lateral connections  \n",
    "C) Neither uses top-down or bottom-up  \n",
    "D) Doesn't use lateral connections  \n",
    "\n",
    "Ans) A \n",
    "\n",
    "\n",
    "3) Which of the following statements are True about RetinaNet?  \n",
    "A) P3 is responsible for detecting small objects compared to P4, P5, P6, P7  \n",
    "B) P2 is computationally expensive and is ignored.  \n",
    "C) p2 has semantically rich features but poor localized features   \n",
    "D) p7 has locally rich features but poor semantically rich features  \n",
    "\n",
    "Ans) A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-3 Anchor boxes. \n",
    "\n",
    "### 3.1 Anchor Boxes\n",
    "As we have discussed previous chapters on the design of anchor boxes, lets reiterate here a bit. Instead of having standard sliding window anchor boxes, we modified anchor boxes a bit primarily for two reasons \n",
    "\n",
    "1) The objects come in different shapes . So having the same grid size will make it difficult for the network to learn and identify objects of different sizes and shapes. In the following image, the car aspect ratio (width/height) is different from human aspect ratio.  \n",
    "\n",
    "![Images with car and human](../images/anchor_02.png)\n",
    "\n",
    "2) The objects comes in different sizes. A small object, medium object and a large object (Comes with different sizes). For example, see the following image where cheetah is present at different distances from the camera. Now instead of 1 anchor, if we have 3 anchors  at each location, each one will be responsible for detecting small, medium and large objects respectively, which will make the network training simpler.\n",
    "![cheetah](../images/cheetah.jpg)\n",
    "\n",
    "\n",
    "So 3 different sizes along with 3 different aspect ratios, together a total of 9 anchors at each location will help us in detecting objects easily and also help the network to get trained faster with higher accuracy. Following this we will see how anchors are designed in RetinaNet.\n",
    "\n",
    "\n",
    "### 3.2 RetinaNet anchor boxes \n",
    "- When designing anchor boxes as mentioned in the network, we have 5 stride values 8, 16, 32, 64, 128 on which we have to design anchors. The lower strides which comes with lower **sizes** are responsible for detecting small objects and the higher strides which comes with higher **sizes** are responsible for detecting large objects. \n",
    "- At each stride, using ratios and scales mentioned below we get 9 anchors. Since we have 5 strides we get a total of 45 anchors. Though each stride helps in identifying small, medium and large objects, we still use scales to further distinguish sizes within these small, medium and large objects to detect objects at all the scales with high accuracy. The following are the most important params for designing anchor boxes.\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "sizes = [32, 64, 128, 256, 512]\n",
    "strides = [8, 16, 32, 64, 128]\n",
    "ratios = np.array([0.5, 1, 2])\n",
    "scales = np.asarray([2**0, 2**(1.0/3.0), 2**(2.0/3.0)])\n",
    "```\n",
    "\n",
    "RetinaNet uses 9 anchor box per grid cell instead of 1 which we mentioned before. At each location, we will have 9 anchor boxes of different ratios and scales. Lets generate anchors for **stride 8**. The feature map size is 100 * 100. So there should be a total of (90000) 100 * 100 * 9 anchors. The following is the procedure we follow to generate the anchors for all the grid cells.\n",
    "\n",
    "- We select the feature map for generating anchors and based on that select stride and size of the anchors. For feature map of size (100, 100) the size is 32 and stride is 8.\n",
    "- We generate anchors at cell_00\n",
    "- For generating anchors at all the cells\n",
    "- We generate anchors at all the grid cells and concatenate them to get one array.\n",
    "- We repeat the same for other strides.\n",
    "\n",
    "\n",
    "1) Lets get the shapes of feature maps \n",
    "\n",
    "```python\n",
    "from keras_retinanet.utils.anchors import guess_shapes\n",
    "image_shape = (800, 800, 3)\n",
    "feature_map_shapes = guess_shapes(image_shape, [3, 4, 5, 6, 7])\n",
    "print(feature_map_shapes)\n",
    "\n",
    "#Out: [array([100, 100]), array([50, 50]), array([25, 25]), array([13, 13]), array([7, 7])]\n",
    "````\n",
    "\n",
    "2) We will choose the first element in the list. If we generate anchors for this feature map, we can run a for loop and generate feature map for others as well.\n",
    "\n",
    "```python\n",
    "fe_size = feature_map_shapes[0]\n",
    "size = sizes[0]\n",
    "stride = strides[0]\n",
    "print(fe_size, size, stride)\n",
    "\n",
    "# [100, 100], 32, 8\n",
    "```\n",
    "\n",
    "\n",
    "3) Generate anchors with Center (0, 0)\n",
    "\n",
    "```python \n",
    "from keras_retinanet.utils.anchors import generate_anchors\n",
    "anchors = generate_anchors(size, ratios, scales)\n",
    "print(anchors)\n",
    "\n",
    "# Out\n",
    "# [[-22.627417   -11.3137085   22.627417    11.3137085 ]\n",
    "#  [-28.50875898 -14.25437949  28.50875898  14.25437949]\n",
    "#  [-35.91878555 -17.95939277  35.91878555  17.95939277]\n",
    "#  [-16.         -16.          16.          16.        ]\n",
    "#  [-20.1587368  -20.1587368   20.1587368   20.1587368 ]\n",
    "#  [-25.39841683 -25.39841683  25.39841683  25.39841683]\n",
    "#  [-11.3137085  -22.627417    11.3137085   22.627417  ]\n",
    "#  [-14.25437949 -28.50875898  14.25437949  28.50875898]\n",
    "#  [-17.95939277 -35.91878555  17.95939277  35.91878555]]\n",
    "```\n",
    "\n",
    "\n",
    "4) Visualizing the anchor boxes using matplotlib \n",
    "\n",
    "```python\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in anchors:\n",
    "    x1, y1, x2, y2 = i\n",
    "    plt.hlines(y1, x1, x2)\n",
    "    plt.hlines(y2, x1, x2)\n",
    "    plt.vlines(x1, y1, y2)\n",
    "    plt.vlines(x2, y1, y2)\n",
    "plt.title(\"Anchors at location (0, 0)\")\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "![anchos at location 0, 0](../images/anchor_at_loc00.png)\n",
    "\n",
    "\n",
    "5) Generate anchors at all the windows. To do this we need to generate all the centers for each window on the feature map \n",
    "\n",
    "```python\n",
    "shift_x = (np.arange(0, fe_size[1]) + 0.5)\n",
    "shift_y = (np.arange(0, fe_size[0]) + 0.5) \n",
    "shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel(),\n",
    "        shift_x.ravel(), shift_y.ravel())).transpose()\n",
    "print(shifts)\n",
    "\n",
    "# array([[ 0.5,  0.5,  0.5,  0.5],\n",
    "#        [ 1.5,  0.5,  1.5,  0.5],\n",
    "#        [ 2.5,  0.5,  2.5,  0.5],\n",
    "#        ...,\n",
    "#        [97.5, 99.5, 97.5, 99.5],\n",
    "#        [98.5, 99.5, 98.5, 99.5],\n",
    "#        [99.5, 99.5, 99.5, 99.5]])\n",
    "```\n",
    "\n",
    "\n",
    "6) Visualizing all the window centers of anchor boxes. Lets use pillow for the same \n",
    "\n",
    "```python\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "img = np.uint8(np.ones((800, 800, 3)))\n",
    "img = Image.fromarray(img) \n",
    "draw = ImageDraw.Draw(img)\n",
    "for i in shifts:\n",
    "    x = i[0]*stride ## Multiplying with stride to visualize this on image\n",
    "    y = i[1]*stride\n",
    "    draw.point((x, y), fill=\"red\")\n",
    "\n",
    "```\n",
    "\n",
    "![Window centers of anchor boxes](../images/centers_img.png)\n",
    "\n",
    "7) Now your job is too move our 9 anchors to each of this center. To aggreate and do all of this keras_retinanet has a function called **shift**. Using that we get all the 90000 anchors.\n",
    "```python \n",
    "from keras_retinanet.utils.anchors import shift\n",
    "shifted_anchors = shift(fe_size, stride, anchors)\n",
    "print(shifted_anchors.shape)\n",
    "\n",
    "##Out: (90000, 4)\n",
    "```\n",
    "8) Lets visulaize some of these anchors using pillow \n",
    "\n",
    "```python\n",
    "for i in shifted_anchors[5049*9:5050*9]:\n",
    "    draw.rectangle([(i[0], i[1]), (i[2], i[3])], fill=None, width=1)\n",
    "```\n",
    "\n",
    "![centers_with_anchors](../images/centers_with_anchors.png)\n",
    "\n",
    "We can clearly see in the above image on how anchors are generated over the image. So now lets generate anchors on all the feature maps and combine them to one vector so that we can procced further. \n",
    "\n",
    "\n",
    "```python\n",
    "from keras_retinanet.utils.anchors import AnchorParameters, guess_shapes\n",
    "print(AnchorParameters.default.sizes)\n",
    "print(AnchorParameters.default.scales)\n",
    "print(AnchorParameters.default.ratios)\n",
    "print(AnchorParameters.default.strides)\n",
    "\n",
    "# Out\n",
    "# [32, 64, 128, 256, 512]\n",
    "# [1.        1.2599211 1.587401 ]\n",
    "# [0.5 1.  2. ]\n",
    "# [8, 16, 32, 64, 128]\n",
    "```\n",
    "\n",
    "- AnchorParamters contains all the default values we have talked about in this section\n",
    "- guess_shape takes in image size as input and calculate the feature map sizes for this network. Note that if network changes, we need write our own guess_shape function. changing the network is out of scope for this course.\n",
    "\n",
    "The keras_retinanet package has a function called anchors_for_shape, which generates all the anchors for a given image size. Though we have used (800, 800) as default here, We can use images of any size. keeping that in mind, lets code and see how it actually works.\n",
    "\n",
    "```python\n",
    "from keras_retinanet.utils.anchors import anchors_for_shape\n",
    "```\n",
    "\n",
    "The anchors for shape takes in the following params\n",
    "\n",
    "| Arguments | Default | Description |\n",
    "| ----------------|------------| -----------------|\n",
    "| image_shape| - | Tuple, the shape of the input image|\n",
    "| pyrmaid_levels| - | List of pyramids to use|\n",
    "| anchor_params| None| If None, it internally uses default values mentioned in AnchorParameters|\n",
    "| shapes_callback| None | Function which calculates the feature map sizes. If None uses default guess_shapes function|\n",
    "\n",
    "\n",
    "```python\n",
    "image_size = (800, 800)\n",
    "pyramid = [3, 4, 5, 6, 7]\n",
    "all_anchors = anchors_for_shape(image_size, pyramid, AnchorParameters.default, guess_shapes)\n",
    "print(all_anchors.shape)\n",
    "#out: (120087, 4)\n",
    "```\n",
    "\n",
    "Generation of anchors is done. We now need to assign ground truth to each anchor and create a classification_target and regression_target so that we can train the network. we will take up this in our next section **Encoding Ground Truths**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "Consider we have an image of size (960, 960). Using default Anchor parameters as mentioned in keras.retinanet.utils.anchors.AnchorParameters.default. \n",
    "\n",
    "Using the parameters defined above. Answer the following question\n",
    "\n",
    "Q1) What are the feature map sizes for P3, P4, P5?\n",
    "\n",
    "Instructions:\n",
    "- use guess_shapes in keras_retinanet.utils.anchors\n",
    "\n",
    "Ans) [array([120, 120]), array([60, 60]), array([30, 30])]  \n",
    "\n",
    "Solution)\n",
    "```python\n",
    "from keras_retinanet.utils.anchors import guess_shapes\n",
    "image_shape = (960, 960, 3)\n",
    "feature_map_shapes = guess_shapes(image_shape, [3, 4, 5])\n",
    "print(feature_map_shapes)\n",
    "```\n",
    "\n",
    "How many anchors are generated on P3?\n",
    "\n",
    "Instructions:\n",
    "- Use only P3 feature map \n",
    "- Use anchors_for_shape in keras_retinanet.utils.anchors\n",
    "\n",
    "Ans) 129600\n",
    "\n",
    "Solution)\n",
    "```python\n",
    "from keras_retinanet.utils.anchors import anchors_for_shape\n",
    "image_size = (960, 960)\n",
    "pyramid = [3]\n",
    "all_anchors = anchors_for_shape(image_size, pyramid, AnchorParameters.default, guess_shapes)\n",
    "print(all_anchors.shape)\n",
    "```\n",
    "\n",
    "How many anchors are present if we decided not to use P2 ?\n",
    "\n",
    "Ans) 43101\n",
    "\n",
    "solution)\n",
    "\n",
    "```python\n",
    "from keras_retinanet.utils.anchors import anchors_for_shape\n",
    "image_size = (960, 960)\n",
    "pyramid = [4, 5, 6, 7]\n",
    "all_anchors = anchors_for_shape(image_size, pyramid, AnchorParameters.default, guess_shapes)\n",
    "print(all_anchors.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-4 Encoding groud truth box\n",
    "\n",
    "In the last module we have generated ~120087 anchors for an image with size (800, 800) which have 5 pyrmaids (p3, p4, p5, p6, p7). In this module we have to assign co-oordinate labels (4) and class labels (80 for coco) for each anchor. We will learn how we are going to do that.  Lets say we have an image of size (800, 800) and it has an object [200, 300, 400, 500]. \n",
    "Previously we have seen that, each grid cell has only one anchor and the ground truth is assigned to the grid cell which contains the object center. RetinaNet uses a much simpler approach. It calculates the intersection of each anchor with the object, it assigns the ground truth object to the anchor which has max intersection (IOU). \n",
    "\n",
    "```python\n",
    "from keras_retinanet.utils.compute_overlap import compute_overlap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "img = np.uint8(np.ones((800, 800, 3)))\n",
    "img = Image.fromarray(img) \n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "gt_box = np.asarray([200, 300, 400, 500]).reshape(1, -1) # A dummy anchor box \n",
    "for i in gt_box:\n",
    "    draw.rectangle([(i[0], i[1]), (i[2], i[3])], fill=None, width=1, outline=\"red\")\n",
    "```\n",
    "\n",
    "![image with bbox](../images/bbox_img.png)\n",
    "\n",
    "Lets compute the iou between anchors and gt_boxes. We can visualize the same using matplotlib\n",
    "\n",
    "```python\n",
    "overlaps = compute_overlap(all_anchors.astype(np.float64), gt_box.astype(np.float64))\n",
    "print(\"Max overlap with any anchor: \", overlaps.max())\n",
    "\n",
    "## Out: Max overlap with any anchor:  0.9464433477433575\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.hist(overlaps)\n",
    "plt.show()\n",
    "```\n",
    "![Histogram Overlap](../images/histogram_overlap.png)\n",
    "\n",
    "\n",
    "We can see that majority of the anchor boxes are not intersecting with the object. The max overlap with any anchor is ~0.94. lets visulize the max_iou anchor \n",
    "\n",
    "```python\n",
    "max_iou_anchor_box = all_anchors[np.argmax(overlaps, axis=0)]\n",
    "print(max_iou_anchor_box)\n",
    "\n",
    "# [[202.40633392 298.40633392 405.59366608 501.59366608]]\n",
    "\n",
    "for i in max_iou_anchor_box:\n",
    "    draw.rectangle([(i[0], i[1]), (i[2], i[3])], fill=None, width=3, outline=\"green\")\n",
    "    \n",
    "```\n",
    "\n",
    "![Bbox ground truth anchor box](../images/bbox_gt_anchor.png)\n",
    "\n",
    "\n",
    "As we can clearly see, the anchor box in green color has high iou with the object. Now, lets initialize two numpy arrays, one for classification and the other for regression.\n",
    "\n",
    "```python\n",
    "num_classes = 80 # consider it is a coco dataset\n",
    "regression_batch  = np.zeros(( all_anchors.shape[0], 4 + 1))\n",
    "labels_batch = np.zeros((all_anchors.shape[0], num_classes + 1))\n",
    "print(regression_batch.shape, labels_batch.shape)\n",
    "\n",
    "##(120087, 5) (120087, 81)\n",
    "```\n",
    "\n",
    "The regression batch contains 4 indices to predict x, y, w, h of the object. The final index tells whether the anchor should be used to calculate the loss (1) or not (-1), called the ignore index. The labels batch contain n_classes (row) and 1 row again to tell whether the anchor should be used to calculate the loss (1) or not (-1), called the ignore index.\n",
    "\n",
    "\n",
    "### Rules for assigning gt_box to the ground truth\n",
    "- if an anchor has iou >= 0.5, the anchor is positive anchor.\n",
    "- if an anchor has 0.4 <= iou < 0.5, the anchor needs to be ignored ( we will shortly understand the reason)\n",
    "- if an anchor has iou < 0.4, it is a negative anchor\n",
    "\n",
    "\n",
    "#### Case-1: if an anchor has iou >= 0.5, the anchor is positive anchor.\n",
    "Lets visualize the anchors which have pos_iou with the gt_box. \n",
    "\n",
    "\n",
    "```python\n",
    "pos_index = (overlaps >= 0.5).reshape(-1)\n",
    "pos_anchors = all_anchors[pos_index]\n",
    "print(pos_anchors.shape)\n",
    "#(49, 4)\n",
    "```\n",
    "\n",
    "visualizing the pos anchor boxes\n",
    "\n",
    "```python\n",
    "for i in pos_anchors:\n",
    "    draw.rectangle([(i[0], i[1]), (i[2], i[3])], fill=None, width=1, outline=\"yellow\")\n",
    "```\n",
    "\n",
    "![pos boxes](../images/pos_boxes.png)\n",
    "\n",
    "As we can see these all anchors have over lap with gt_box. In retinanet, the authors have considered iou >= 0.5 as pos anchor. means all these anchor boxes has the chance to contain an object. \n",
    "\n",
    "\n",
    "#### Case-2 if an anchor has 0.4 <= iou < 0.5, the anchor needs to be ignored\n",
    "\n",
    "```python\n",
    "ignore_index = (overlaps < 0.5).reshape(-1) & (overlaps > 0.4).reshape(-1)\n",
    "ignore_anchors = all_anchors[ignore_index]\n",
    "print(ignore_anchors.shape)\n",
    "#out: (56, 4)\n",
    "```\n",
    "\n",
    "There are a total of 56 ignore anchors. We have to ignore these anchors while training because, they partly contain objects and therefore we can neither tell what object is present within the anchor nor we can discard that as background as partially there is some object present. So hence ignored. to visualize use the following code.\n",
    "\n",
    "```python\n",
    "img = np.uint8(np.ones((800, 800, 3)))\n",
    "img3 = Image.fromarray(img).copy()\n",
    "draw = ImageDraw.Draw(img3)\n",
    "\n",
    "for i in ignore_anchors:\n",
    "    draw.rectangle([(i[0], i[1]), (i[2], i[3])], fill=None, width=1, outline=\"brown\")\n",
    "\n",
    "for i in gt_box:\n",
    "    draw.rectangle([(i[0], i[1]), (i[2], i[3])], fill=None, width=5, outline=\"red\")\n",
    "```\n",
    "\n",
    "![ignore boxes](../images/ignore_boxes.png)\n",
    "\n",
    "\n",
    "#### Case-3 if an anchor has iou < 0.4, it is a negative anchor\n",
    "- Anchors with iou < 0.4 are considered to be negative (background). Since the numpy arrays are initialized with zeros, all the anchors other than pos_anchors and ignore anchors are considered negative anchors\n",
    "\n",
    "\n",
    "### How the classification and regression targets are created ?\n",
    "\n",
    "we have the following things with us \n",
    "\n",
    "\n",
    "| Arguments | Shape | Description |\n",
    "| ----------------|------------| -----------------|\n",
    "| labels_batch| (120087, 81) | 80 classes for labels and 1 for indexing weather it is a pos, neg or ignore anchor|\n",
    "| regression_batch| (120087, 5) | 4 coordinates and 1 for indexing weather it is a pos, neg or ignore anchor| \n",
    "| pos_index| -| True for the anchors which are positive|\n",
    "| ignore_index| - | True for the anchors which needs to be ignored|\n",
    "\n",
    "since we have initialized both **labels_batch** and **regression_batch** with zeros, we need to replace pos_anchors with 1 and ignore_anchors with -1.\n",
    "\n",
    "\n",
    "```python\n",
    "labels_batch[ignore_index, -1]       = -1\n",
    "labels_batch[pos_index, -1]     = 1\n",
    "\n",
    "regression_batch[ignore_index, -1]   = -1\n",
    "regression_batch[pos_index, -1] = 1\n",
    "```\n",
    "\n",
    "suppose the class of the object is 20, then for the postive_indices, we need to replace the 20th row with 1's.\n",
    "\n",
    "```python\n",
    "labels_batch[pos_index, 20] = 1\n",
    "```\n",
    "\n",
    "\n",
    "For the regression outputs x1, y1, x2, y2 for each anchor are obtained relative to the anchor box and lets call these tx1, ty1, tx2, ty2.\n",
    "```\n",
    "anchor_height = ty2 - ty1\n",
    "anchor_width = tx2 - tx1 \n",
    "\n",
    "dx1 = (x1 - tx1)/ anchor_width\n",
    "dx2 = (x2 - tx2)/ anchor_width\n",
    "dy1 = (y1 - ty1)/ anchor_height\n",
    "dy2 = (y2 - ty2)/ anchor_height\n",
    "\n",
    "targets = np.stack((dx1, dy1, dx2, dy2))\n",
    "targets = targets.T \n",
    "\n",
    "targets = (target - mean)/std ## normalizing the predictions.\n",
    "```\n",
    "In keras_retinanet there is a function called bbox_transform which actually does this for us.\n",
    "\n",
    "```python\n",
    "from keras_retinanet.utils.anchors import bbox_transform\n",
    "regression_batch[:, :-1] = bbox_transform(all_anchors, gt_box)\n",
    "```\n",
    "\n",
    "Though we have calculated these for all the anchors, the last index will tell which anchors are positive, so while calculating loss functions we can use that index to ignore other labels.\n",
    "\n",
    "\n",
    "The final two numpy arrays **labels_batch** and **regression_batch** are used as targets for training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "For a ground truth object at [20, 30, 200, 300] with an image size of (960, 960) calculate the following things.\n",
    "\n",
    "1) max_iou with any anchor (upto two decimals)  \n",
    "2) anchor with max_iou  \n",
    "3) total number of pos_anchors if pos_threshold is 0.5 \n",
    "4) total number of negitive anchors (total_anchors - pos_anchors - ignore_anchors)  \n",
    "\n",
    "Instructions: \n",
    "- Use default anchorparameters.\n",
    "\n",
    "Solutions:  \n",
    "1) 0.748\n",
    "```python\n",
    "from keras_retinanet.utils.anchors import anchors_for_shape\n",
    "from keras_retinanet.utils.compute_overlap import compute_overlap\n",
    "import numpy as np\n",
    "\n",
    "image_size = (960, 960)\n",
    "pyramid = [3, 4, 5, 6, 7]\n",
    "all_anchors = anchors_for_shape(image_size, pyramid, AnchorParameters.default, guess_shapes)\n",
    "print(all_anchors.shape)\n",
    "\n",
    "\n",
    "x = [20, 30, 200, 300]\n",
    "gt_box = np.asarray(x).reshape(1, -1)\n",
    "\n",
    "overlaps = compute_overlap(all_anchors.astype(np.float64), gt_box.astype(np.float64))\n",
    "print(\"Max overlap with any anchor: \", overlaps.max())\n",
    "\n",
    "```\n",
    "\n",
    "2) [ 40.16242979,  32.32485958, 183.83757021, 319.67514042]\n",
    "\n",
    "```python\n",
    "max_iou_anchors = all_anchors[overlaps.argmax()]\n",
    "print(max_iou_anchors)\n",
    "```\n",
    "\n",
    "3) 35\n",
    "```python\n",
    "pos_index = (overlaps >= 0.5).reshape(-1)\n",
    "pos_anchors = all_anchors[pos_index]\n",
    "print(pos_anchors.shape)\n",
    "```\n",
    "\n",
    "4) 172607\n",
    "```python\n",
    "pos_index = (overlaps >= 0.5).reshape(-1)\n",
    "pos_anchors = all_anchors[pos_index]\n",
    "print(pos_anchors.shape)\n",
    "\n",
    "\n",
    "ignore_index = (overlaps < 0.5).reshape(-1) & (overlaps > 0.4).reshape(-1)\n",
    "ignore_anchors = all_anchors[ignore_index]\n",
    "print(ignore_anchors.shape)\n",
    "\n",
    "neg_anchors = all_anchors.shape[0] - pos_anchors.shape[0] - ignore_anchors.shape[0]\n",
    "print(neg_anchors)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 5 - Loss functions\n",
    "There are two loss functions, one for regression (coordinates) and the other for classification (labels)\n",
    "\n",
    "## Focal loss for classification\n",
    "- For classification, The authors have used a novel loss function called **Focal loss**, which is a remodification to the cross entropy loss function, allowing us to weight mis-classified and hard examples high compared to easily classified examples. \n",
    "\n",
    "Cross_entropy_loss = - log(p_{t})  \n",
    "Focal loss = \\alpha * (1-p_{t})^\\gamma * log(p_{t})  \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "Cross entropy loss = - log(p_{t})  \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Focal loss = \\alpha * (1-p_{t})^\\gamma * log(p_{t}) \n",
    "\\end{equation*}\n",
    "\n",
    "where  \n",
    "\\begin{equation*}\n",
    "p_{t} = (p) \\; if \\; (y = 1) \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "p_{t} = (1- p) \\; if \\; (y =0)  \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "One of the fundamental problems with object detection is that there are too many background anchor boxes compared to foreground anchor boxes (49 pos anchors boxes in a total of ~120000 anchor boxes as seen previously). Since the negitive classes overwhelm the pos classes it is important to give more weight to hard examples rather than easily classifable background classes. The authors of RetinaNet paper have used Focal loss to solve this problem. Lets understand Focal loss by considering the following scenarios. we will use \\gamma value 0.25 and and \\alpha value 2.0.  \n",
    "\n",
    "### Scenario-1: Easy correctly classified example\n",
    "\n",
    "Say we have an easily classified foreground object with p=0.9. Now usual cross entropy loss for this example is\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "CE(foreground) = -log(0.9) = 0.1053  \n",
    "\\end{equation*}\n",
    "\n",
    "Now, consider easily classified background object with p=0.1. Now usual cross entropy loss for this example is again the same\n",
    "\n",
    "\\begin{equation*}\n",
    "CE(background) = -log(1–0.1) = 0.1053\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Now, consider focal loss for both the cases above. We will use alpha=0.25 and gamma = 2\n",
    "\n",
    "\\begin{equation*}\n",
    "FL(foreground) = -1 x 0.25 x (1–0.9)^2 log(0.9) = 0.00026\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "FL(background) = -1 x 0.25 x (1–(1–0.1))^2 log(1–0.1) = 0.00026.\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### Scenario-2: misclassified example\n",
    "\n",
    "Say we have an misclassified foreground object with p=0.1. Now usual cross entropy loss for this example is\n",
    "\n",
    "\\begin{equation*}\n",
    "CE(foreground) = -log(0.1) = 2.3025\n",
    "\\end{equation*}\n",
    "\n",
    "Now, consider misclassified background object with p=0.9. Now usual cross entropy loss for this example is again the same\n",
    "\n",
    "\\begin{equation*}\n",
    "CE(background) = -log(1–0.9) = 2.3025\n",
    "\\end{equation*}\n",
    "\n",
    "Now, consider focal loss for both the cases above. We will use alpha=0.25 and gamma = 2\n",
    "\n",
    "\\begin{equation*}\n",
    "FL(foreground) = -1 x 0.25 x (1–0.1)^2 log(0.1) = 0.4667\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "FL(background) = -1 x 0.25 x (1–(1–0.9))^2 log(1–0.9) = 0.4667\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### Scenario-3: Very easily classified example\n",
    "\n",
    "Say we have an easily classified foreground object with p=0.99. Now usual cross entropy loss for this example is\n",
    "\n",
    "\\begin{equation*}\n",
    "CE(foreground) = -log(0.99) = 0.01\n",
    "\\end{equation*}\n",
    "\n",
    "Now, consider easily classified background object with p=0.01. Now usual cross entropy loss for this example is again the same\n",
    "\n",
    "\\begin{equation*}\n",
    "CE(background) = -log(1–0.01) = 0.1053\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Now, consider focal loss for both the cases above. We will use alpha=0.25 and gamma = 2\n",
    "\n",
    "\\begin{equation*}\n",
    "FL(foreground) = -1 x 0.25 x (1–0.99)^2 log(0.99) = 2.5*10^(-7)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "FL(background) = -1 x 0.25 x (1–(1–0.01))^2 log(1–0.01) = 2.5*10^(-7)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "scenario-1: 0.1/0.00026 = 384 times smaller number\n",
    "\n",
    "scenario-2: 2.3/0.4667 = 5 times smaller number\n",
    "\n",
    "scenario-3: 0.01/0.00000025 = 40,000 times smaller number.\n",
    "\n",
    "These three scenarios clearly show that Focal loss add very less weight to well classified examples and large weight to miss-classified or hard classified examples.\n",
    "\n",
    "This is the basic intuition behind designing Focal loss. The authors have tested different values of alpha and gamma and final settled with the above mentioned values.\n",
    "\n",
    "\n",
    "## Smooth L1 loss for Regression \n",
    "Smooth L1-loss can be interpreted as a combination of L1-loss and L2-loss. It behaves as L1-loss when the absolute value of the argument is high, and it behaves like L2-loss when the absolute value of the argument is close to zero. \n",
    "\n",
    "```\n",
    "f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n",
    "       |x| - 0.5 / sigma^2         otherwise\n",
    "```\n",
    "sigma is a hyperparameter. In this paper the authors have choosen sigma as 3. and x is the difference between true_positive and predicted coordinates. Smooth L1-loss combines the advantages of L1-loss (steady gradients for large values of 𝑥) and L2-loss (less oscillations during updates when 𝑥 is small).\n",
    "\n",
    "\n",
    "Since, we understood the Network, anchors, encoding targets and loss functions, we can now go and proceed and train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Quiz\n",
    "\n",
    "Q) Which loss function is good for classification loss in RetinaNet ?  \n",
    "A) Focal loss   \n",
    "B) Cross entropy loss   \n",
    "C) Either A or B  \n",
    "D) neither A nor B  \n",
    "\n",
    "\n",
    "Ans) A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
