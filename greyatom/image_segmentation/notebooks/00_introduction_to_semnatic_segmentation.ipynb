{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter-1 Introduction to semantic segmentation\n",
    "\n",
    "- Topic - What is segmentation?\n",
    "- Topic - Difference between thing and stuff.\n",
    "- Topic - Different types of segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic1\n",
    "## 1.1 - What is segmentation ?\n",
    "One of the primary goals of computer vision is the understanding of visual scenes. Scene understanding involves numerous tasks including recognizing what objects are present, localizing the objects in 2D and 3D,determining the objects and scene’s attributes, characterizing relationships between objects and providing a semantic description of the scene. Inorder to do these we need to understand what every pixel in a image represents and so **Image segmentation** is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics. For example in the medical industry, while dealing with cancer cells detection, it is very important for a Radiologist to segment each and every pixel of CT-scan as cancer or non-cancer cell. This will allow the doctor to assess the damage and suggest a medication based on that. For we as a data scientist our fundamental duty is to separate out the cancerous cells(1) from the background (0). This is an image-segmentation problem. \n",
    "\n",
    "## 1.2 How is image segmenatation different from image classification?\n",
    "As shown in the images below, In image classification, we assign a label (single classification problem) or a set of labels (multi-class classification) to each image. In Image segmentation, we assign a label to each and every pixel of the image. For example, the image shown below contains multiple labels, the image classification outputs 3 lables called surfboard, human and water. It doesn't tell where water, human or surf board is located inside the image. In image segmentation, if the image height and width is (400x600) we get an output of same size, with each pixel contain the label index of the class. In this case we have water (0), surf_board(1) and humans(2). This not only tells what objects are present, it also tells where the objects are present inside the image and how much space do they occupy. This level information will further help us in scene parasing and understanding.\n",
    "\n",
    "![surf_class](../images/surf_class.png)\n",
    "![surf_seg](../images/surf_seg.png)\n",
    "\n",
    "## 1.3 Use cases \n",
    "Image segmentation is extensively used in several industries. Some of the domains include.  \n",
    "\n",
    "- In the medical domain, We can see Companies like [Sigtuple](https://sigtuple.com/#s-solutions), [Qure.ai](http://qure.ai), [PathAI](https://www.pathai.com/) heavily using image segmenation in their solutions.\n",
    "\n",
    "- In GeoSensing – For land usage, Land cover information is important for various applications, such as monitoring areas of deforestation and urbanization.To recognize the type of land cover (e.g., areas of urban, agriculture, water, etc.) for each pixel on a satellite image, land cover classification can be regarded as a multi-class semantic segmentation task. Road and building segmentation is also an important research topic for traffic management, city planning, and road monitoring. This satellite data is also used by traders to get better insights on the business. For example, a coal trader may want to know when demand is going to exceed supply, driving up prices. Satellite images could measure the piles of coal outside power plants to see when they’re getting low and need to be refilled. They can count trains and track ocean tankers carrying supplies. They can measure ground levels, pile heights and truck activities at mines to deduce production. Now given a satellite image, if we can segment the coal portion, or its shadow from other parts of the image, we can convert the unstructed image data to structured (%of space coal occupied) data.\n",
    "\n",
    "Likewise segmentation has its inroads into many industries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-2 Thing and Stuff Objects \n",
    "In the vision community, we classify objects as stuff and things. \n",
    "\n",
    "![thing-stuff-image](../images/thing_stuff_img.png)\n",
    "\n",
    "### 2.1 Thing\n",
    "Things are objects with a specific size and shape, that are often composed of parts.\n",
    "For examples, persons, cars, bikes etc belong to thing category. In the above example shown, human and aircraft are considered as thing objects.\n",
    "\n",
    "### 2.2 stuff \n",
    "Stuff classes are background materials that are defined by homogeneous or repetitive patterns of fine-scale properties, but have no specific or distinctive spatial extent or shape. In the above example shown, sky and trees are considered as stuff objects.  \n",
    "\n",
    "Why the focus on stuff? Stuff covers about 66% of the pixels in COCO (Dataset which we will use later). It allows us to explain important aspects of an image, including scene type; which thing classes are likely to be present and their location; as well as geometric properties of the scene.  \n",
    "\n",
    "### 2.3 Defining  things  and  stuff.\n",
    "The  literature  provides  definitions  for  several  aspects  of  stuff  and  things,  including:    \n",
    "\n",
    "- **Shape:** Things have characteristic shapes (car,cat,phone), whereas stuff is amorphous (sky, grass, water)\n",
    "- **Size:** Things occur at characteristic sizes with little variance, whereas stuff regions are highly variable in size.\n",
    "- **Parts:** Thing classes have identifiable parts, where as stuff classes do not (e.g. a piece of grass is still grass,but a wheel is not a car).   \n",
    "- **Instances:**  Stuff classes are typically not countable and have no clearly  defined  instances. \n",
    "- **Texture:** Stuff classes are typically highly textured.  \n",
    "\n",
    "Finally, a few classes can be interpreted as both stuff and things, depending on the image conditions (e.g. a large number of people is sometimes considered a crowd). Several works have shown that different techniques are required for the detection of stuff and things. Moreover, several works have shown that stuff is a useful contextual cue to detect things and vice versa.\n",
    "\n",
    "Based on detection of Thing and stuff in an image at various levels, The image segmentation tasks in vision communinty are broadly classified into three categories. They are called, \n",
    "- Semantic segmentation\n",
    "- Instance segmentation\n",
    "- Panoptic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "1) Which of the following label names belong to stuff ? (One or more answers possible)  \n",
    "A) Road  \n",
    "B) Sky  \n",
    "C) playingfield  \n",
    "D) Football  \n",
    "\n",
    "\n",
    "Ans) A, B, C (Road, SKY and Playingfield is not countable, They donot have definative shape and size, where as footballs can be counted and are round in shape and have definative size).  \n",
    "\n",
    "2) Which of the following label names belong to thing? (One or more answers possible)  \n",
    "A) River  \n",
    "B) Kite  \n",
    "C) Clouds  \n",
    "D) Surf board   \n",
    "\n",
    "Ans) B, D (Both Kite and Surf board can be counted, have definative shape and size)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-3 Different types of segmentation\n",
    "As discussed above, Image segmentation tasks in vision community is broadly classified into three categories. They are called\n",
    "- semantic segmentation\n",
    "- instance segmentation\n",
    "- panoptic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Semantic Segmentation\n",
    "Studying stuff is most commonly formulated as a task known as semantic segmentation, As stuff is amorphous and uncountable, this task is defined as simply assigning a class label to each pixel in an image. Some of the applications including\n",
    "- estimating cancerous cells volume and location in a lung CT scan for automatic diagnosis of Cancer.\n",
    "- Automatically and accurately identify if a subsurface target is salt or not. [link](https://www.kaggle.com/c/tgs-salt-identification-challenge)\n",
    "- The dataset usually contains an input image (RGB mostly) and the output contains a class number for each pixel. In the below image we can see that each stuff and thing object is given a different pixel value. Note: Frizbee, bench and humans though appearing black, were actually given different pixel values, this can be seen in the below bar chart.\n",
    "\n",
    "input image             |  semantic image\n",
    ":-------------------------:|:-------------------------:\n",
    "![input image](segmentation/input_image.png)  |  ![semantic image](segmentation/semantic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to count the number of pixels each class is occupying? We will be using **labels.txt** for this excerise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Load the image\n",
    "img = np.asarray(Image.open(\"segmentation/semantic.png\"))\n",
    "\n",
    "## Return each pixel count\n",
    "pixel, counts = np.unique(img, return_counts = True)\n",
    "\n",
    "## Read the label names, this will give us a dictonary from num (0) -- label_name (person)\n",
    "labels_dict = {int(i.split()[0]): i.split()[1] for i in open(\"labels.txt\", \"r\")}\n",
    "\n",
    "## Collect all the pixel values expect 0 (Pixels that do not belong to any of the other classes, as it is considered as border pixels.)\n",
    "pixel_labels = [labels_dict[i-1] for i in pixel if i!= 0]\n",
    "\n",
    "## Collect the pixel counts for each class except boarder class\n",
    "counts = [counts[num] for num, i in enumerate(pixel) if i!= 0]\n",
    "\n",
    "## Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(pixel_labels, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', 'bench', 'frisbee', 'bush', 'grass', 'river', 'sky-other', 'tree']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task \n",
    "**What is the relative percentage pixels occupied by stuff and thing objects in the image. ?**\n",
    "\n",
    "Instructions to calculate\n",
    "- Ignore boarder pixels \n",
    "- tree, sky-other, river, grass and bush are stuff objects. \n",
    "- bench, person, frisbee are thing objects\n",
    "\n",
    "(a) the objective of performing the task i.e. what will the learner perform and gain out of that task?\n",
    "When we said 66% of the pixels in the COCO dataset belong to stuff objects, How do we calculate that? In this task we will calculate it for one image. Going further learners can apply this on the entire dataset.\n",
    "\n",
    "(b) a set of instructions \n",
    "- make a list of stuff objects **stuff_object_names** and count of pixels of each stuff object as another list **stuff_object_pixels_count**.\n",
    "- repeat the same for thing objects **thing_object_names** and **thing_object_pixels_count**\n",
    "- calculate stuff percentage sum(stuff_object_pixels_count) / (sum(stuff_object_pixels_count) + sum(thing_object_pixels_count))\n",
    "- calculate thing percentage sum(thing_object_pixels_count) / (sum(stuff_object_pixels_count) + sum(thing_object_pixels_count))\n",
    "\n",
    "(c) the solution code. \n",
    "```python\n",
    "stuff_object_names = [\"tree\", \"sky-other\", \"river\", \"grass\", \"bush\"]\n",
    "thing_object_names = [\"bench\", \"person\", \"frisbee\"]\n",
    "\n",
    "stuff_object_pixels_count = [counts[pixel_labels.index(i)] for i in stuff_object_names]\n",
    "print(\"stuff_object_names:{}, stuff_object_pixels_count: {}\".format(stuff_object_names, stuff_object_pixels_count))\n",
    "\n",
    "thing_object_pixels_count = [counts[pixel_labels.index(i)] for i in thing_object_names]\n",
    "print(\"thing_object_names:{}, thing_object_pixels_count: {}\".format(thing_object_names, thing_object_pixels_count))\n",
    "\n",
    "\n",
    "total_stuff_object_pixels_count = sum(stuff_object_pixels_count)\n",
    "print(\"total stuff object pixels count: {}\".format(total_stuff_object_pixels_count))\n",
    "\n",
    "total_thing_object_pixels_count = sum(thing_object_pixels_count)\n",
    "print(\"total thing object pixels count: {}\".format(total_thing_object_pixels_count))\n",
    "\n",
    "print(\"stuff_percentage: {}\".format(total_stuff_object_pixels_count/ (total_stuff_object_pixels_count+total_thing_object_pixels_count)))\n",
    "print(\"thing_percentage: {}\".format(total_thing_object_pixels_count/ (total_stuff_object_pixels_count+total_thing_object_pixels_count)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Instance Segmentation\n",
    "Studying things is typically formulated as the task of instance segmentation, where the goal is to detect each object and delineate it with a segmentation mask, respectively. While semantic segmentation fails in calculating how many objects (example cars) are present in a image(image if two cars are overlapping in a image. It is difficult to separate those 2 cars and count them), Instance segmentation will draw a bounding box and a segmentation mask to each and every object thus allowing to separate each object. The below uses cases explains this clearly\n",
    "-  The kaggle Data science bowl 2018 challege deals with creating an algorithm to automate nucleus detection[Link](https://www.kaggle.com/c/data-science-bowl-2018). As shown in the below diagram, we not only need to find the location of nucleus cell but also separate out each and every cell (so that we can count).\n",
    "![kaggle data science bowl](../images/nucleas.png)\n",
    "- Usuall instance segmentation dataset contains an input image (RGB mostly) and the mask for each and every object separately (if there are 10 objects, there will be 10 masks). In the below image, we can use bounding boxes to separate both the humans (2 instances). \n",
    "\n",
    "input image             |  Instance image\n",
    ":-------------------------:|:-------------------------:\n",
    "![input image](segmentation/input_image.png)  |  ![instance image](segmentation/mask_rcnn_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Panoptic segmentation\n",
    "Can there be a reconciliation between stuff and things? These questions are particularly important given their relevance in real world applications, such as autonomous driving or augmented reality. Datasets like Cityscapes, ADE20K and COCO dataset(with their latest release of panpotic annotations) are especially designed for this. Panoptic segmenation is a unified framework for understanding an image completely. It can detect cars at pixel level and distinguish between different cars, It can separate roads from footpaths etc.\n",
    "\n",
    "- The dataset contains an input image (RGB mostly), an output mask image which contains the class number for each pixel and the mask for each and every object separately (if there are 10 objects, there will be 10 masks). In the below image we can clearly see that we can distingush between stuff and thing objects. Even within thing objects we can clearly distiguish between different person (different colors assigned).  \n",
    "\n",
    "| input image             |  Panoptic image |\n",
    ":-------------------------:|:-------------------------:\n",
    "![input image](segmentation/input_image.png)  |  ![panoptic image](segmentation/panoptic.png)\n",
    "\n",
    "\n",
    "### 3.4 Goals in this course\n",
    "Thought the ultimate goal of the vision community is to make panoptic segmentation feasible, There are several reasons why this is difficult.\n",
    "- Firstly, It is extremely difficult to label everything in a image.\n",
    "- Sencond, the networks pipelines will become complex. It will take higher times to train and validate the results.\n",
    "- There is no need to panoptic segmenation for many uses cases. Also every usecase might not require segmenting each and everything in the image. For example, checkout the things we have discussed in semantic segmenation and instance segmenation. It is unnecessary to make things complicated when we can achieve something very simple. \n",
    "\n",
    "Now since we understood when to use which type of segmentation. This session is dedicated towards **semantic segmenation.** Understanding this properly should open roads for other things too.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "**Q1)** Given a image and when asked to findout how many tigers are present in it. Which kind of segmentation would you perform?  \n",
    "A) Panoptic segmentation  \n",
    "B) Instance segmentation  \n",
    "C) Semantic segmentation   \n",
    "D) Any of the above  \n",
    "\n",
    "Ans) B) Instance segmentation, since we need to count the number of tigers, there is no need to identify background objects, so no need of panoptic segmentation\n",
    "\n",
    "\n",
    "**Q2)** Given a satellite image you were asked to identify the the percentage of land occupied by water bodies, trees and buildings? Which kind of segmentation would you perform ?  \n",
    "\n",
    "A) Panoptic segmentation  \n",
    "B) Instance segmentation  \n",
    "C) Semantic segmentation  \n",
    "D) None of the above     \n",
    "Ans) C) Sematic segmentation, as we just need to indentify percentage of land occupied for each object.  \n",
    "\n",
    "\n",
    "**Q3)** Given a satellite image you were asked to identify the percentage of land occupied by water bodies and roads in the image? In addition to that, you were also asked to count the number of cars present on the road? Which kind of segmentation would you perform ?\n",
    "\n",
    "A) Panoptic segmentation  \n",
    "B) Instance segmentation  \n",
    "C) Semantic segmentation  \n",
    "D) None of the above     \n",
    "Ans) A) Panoptic segmentation, as we need to count the vechiles and distinguish between water bodies and roads too.    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
