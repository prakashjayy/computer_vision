{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "560ff35e",
   "metadata": {},
   "source": [
    "The recent `Google Landmark Recognition Challenge` [winning solutions](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/359316) has seen many versions of CLIP model being used. GLRC is a zero-short learning task where we need to create a 64D image embedding such that identical images should have similar embedding. \n",
    "\n",
    "\n",
    "## Open source implmentations\n",
    "- https://github.com/mlfoundations/open_clip\n",
    "- https://openai.com/blog/clip/\n",
    "- https://arxiv.org/pdf/2103.00020.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2872d",
   "metadata": {},
   "source": [
    "> Contrastive Language-Image Pretraining aka CLIP reached SOTA accuracy (ResNet-50 supervised model accuracy) on imagenet without using single image from it aka `ZERO shot learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ed56d",
   "metadata": {},
   "source": [
    "> In NLP, GPT-3 is trained on webscale data without supervision (masked language models) and surpassed all tasks when compared to their counterparts trained on high-quality crowd labelled dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab236727",
   "metadata": {},
   "source": [
    "> Clip solves the following major problems\n",
    "- Costly datasets are not required \n",
    "- not `Narrow` anymore. CLIP model is competitive with fully supervised models across 30 datasets including OCR and GEOspatial datasets. \n",
    "- Poor real-world performance is resolved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c883aae8",
   "metadata": {},
   "source": [
    "## CLIP Alogirthm: How CLIP is built?\n",
    "- what dataset is used?\n",
    "- Architecuture and loss function\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31642e51",
   "metadata": {},
   "source": [
    "### Dataset - WIT - WebImageText\n",
    "- CLIP uses 400 million (image, text) pairs crawled from internet. \n",
    "- First all the wikipedia words are taken, these are agumented with bi-grams. only words which occur more than 100 times are taken. Roughly 500,000 queries were considered. Approx 20k (image, text) pairs were selected for each pair. \n",
    "- The resulting dataset has a similar total word count as the WebText dataset used to train GPT-2\n",
    "\n",
    "\n",
    "<img src=\"images/clip_wit_example.png\" alt=\"alt text\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59149629",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "<img src=\"images/clip_pseudo_code.png\" alt=\"alt text\" width=\"400\" align=\"left\"/>\n",
    "\n",
    "CLIP works on the principle that given an image, predict which out of a set of N (32,768) randomly sampled text snippets, was actually paired with it in our dataset.\n",
    "\n",
    "#### Text encoder\n",
    "- BPE (lower-cased byte pair encoding) text embeddings of vocab size 49,152 was used.\n",
    "- transformers were used to encode the text and generate a lower order dimensional representation.\n",
    "- For computational efficiency, the max sequence length was capped at 76.\n",
    "\n",
    "#### image encoder.\n",
    "- ResNet, Efficientnet, ViT - A total of 32 variants were tested.\n",
    "- found ViT-L/14@336px to be working better than everything.\n",
    "\n",
    "\n",
    "#### Training\n",
    "- contrastive loss uses cross entropy for pos (N) and neg pairs (N2-N). shown in the pseudo code.\n",
    "- several hyper-parameters were tested for one epoch. \n",
    "- cosine-scheduling and  large minibatch size of 32,768 were used.\n",
    "- mixed precision was used to save memory and accelarate computation.\n",
    "\n",
    "<img src=\"images/clip_stage1.png\" alt=\"alt text\" width=\"300\" align=\"left\"/>\n",
    "\n",
    "#### inference \n",
    "- For each label in the dataset use a prompt (a photo of {label}) and generate text embedding. \n",
    "- Then each image is passed through the `image encoder` and image embedding. These embeddings are scaled by a temperature parameter $\\tau$ , and normalized into a probability distribution via a softmax.\n",
    "- This is kind of multinomial logistic regression classifier with L2-normalized inputs, L2-normalized weights, no bias, and temperature scaling.\n",
    "\n",
    "<img src=\"images/clip_inference.png\" alt=\"alt text\" width=\"300\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1bb1d",
   "metadata": {},
   "source": [
    "## Prompt Engineering \n",
    "While doing inference, using just the label of the class might not be sufficient as the WebImageText dataset contain phrases describing the image. so the authors have used `a photo of {label}` as prompt for classification. The promot improved the accuracy of imagenet by 1.3%. \n",
    "\n",
    "| Dataset | Prompt|\n",
    "|--------- | ---- |\n",
    "|  Oxford-IIIT Pets | “A photo of a {label}, a type of pet.”|\n",
    "| Food101 | a {type} of food|\n",
    "|  FGVC Aircraft | a {type} of aircraft|\n",
    "| satellite image classification | “a satellite photo of a {label}.”|\n",
    "\n",
    "while doing an ensemble of 80 context prompts for each label in imagenet, they saw an improvement of 5% accuracy on ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab77318",
   "metadata": {},
   "source": [
    "## Processing Cats and dogs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a777318",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (detr)",
   "language": "python",
   "name": "detr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
