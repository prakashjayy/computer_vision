{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4df4fb",
   "metadata": {},
   "source": [
    "#### [CSPNet: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w28/Wang_CSPNet_A_New_Backbone_That_Can_Enhance_Learning_Capability_of_CVPRW_2020_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a71f1da",
   "metadata": {},
   "source": [
    "Why need to implement CSPNet?\n",
    "- Reduces the computational load by 10-20% and has superior accuracy compared to ResNet, DenseNet, ResNext and DenseNet. \n",
    "- easy to integrate within any existing frameworks. \n",
    "- evenly distribute the amount of computation at each layer in CNN so that we can effectively upgrade the utilization rate of each computation unit and thus reduce unnecessary energy consumption. On yolov3, it achieved 80% less computational bottleneck. \n",
    "- Overall required memory cost reduces. \n",
    "- improve the inference speed. \n",
    "\n",
    "The goal of this blogpost is to build\n",
    "- ResNet10\n",
    "- CSPResNet10 \n",
    "\n",
    "and understand the changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21010d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from typing import Union, Type, List, Tuple\n",
    "\n",
    "from timm.models.layers import ConvNormAct, DropPath, create_act_layer, ConvNormActAa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debab57",
   "metadata": {},
   "source": [
    "## minimal basic implementation of Resnet10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_chs, out_chs, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        block_kwargs = {\"norm_layer\":nn.BatchNorm2d, \"act_layer\": nn.ReLU}\n",
    "        self.conv1 = ConvNormAct(in_chs, out_chs, kernel_size=3, stride=stride, padding=1, bias=False, **block_kwargs)\n",
    "        self.conv2 = ConvNormAct(out_chs, out_chs, kernel_size=3, padding=1, bias=False, apply_act=False, **block_kwargs)\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if self.downsample is None:\n",
    "            assert stride == 1, \"stride cannot be more than 1 when downsample is None\"\n",
    "    \n",
    "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
    "        residual=x\n",
    "        out: torch.Tensor = self.conv1(x)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc26d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetBlock(\n",
       "  (conv1): ConvNormAct(\n",
       "    (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNormAct2d(\n",
       "      16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): ConvNormAct(\n",
       "    (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNormAct2d(\n",
       "      16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): Identity()\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resblock = ResNetBlock(16, 16, 1)\n",
    "resblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95dba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 24, 24])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 16, 24, 24))\n",
    "resblock(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ede4e",
   "metadata": {},
   "source": [
    "### ResNetBottleneck \n",
    "- we have 3 conv layer \n",
    "- the feature maps extend from in_chs to out_chs* self.expansion\n",
    "- incase we use stride>1, use downsample to reduce the size of skip connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9978f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_chs, out_chs, stride=1, downsample=None)-> None:\n",
    "        super().__init__()\n",
    "        block_kwargs = {\"norm_layer\":nn.BatchNorm2d, \"act_layer\": nn.ReLU}\n",
    "        self.conv1 = ConvNormAct(in_chs, out_chs, kernel_size=1, bias=False, **block_kwargs)\n",
    "        self.conv2 = ConvNormAct(out_chs, out_chs, kernel_size=3, stride=stride, padding=1, bias=False, **block_kwargs)\n",
    "        self.conv3 = ConvNormAct(out_chs, out_chs*self.expansion, kernel_size=1, bias=False, **block_kwargs, apply_act=False )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        if self.downsample is None:\n",
    "            self.downsample = ConvNormAct(in_chs, out_chs*self.expansion, kernel_size=1, stride=self.stride, **block_kwargs, apply_act=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "\n",
    "        out: torch.Tensor = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec13231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 24, 24])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 16, 24, 24))\n",
    "ResNetBottleneck(16, 64, 1)(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3c2ad",
   "metadata": {},
   "source": [
    "### CSPStage\n",
    "- This is an extension to both blocks discussed before\n",
    "- First we take a tensor\n",
    "    - conv_down = if stride>1, we reduce the size of the feature_map using convlayer else use nn.Identity\n",
    "    - conv_exp = increase the number channels based on expand_ratio.\n",
    "    - next we split the tensor into two parts `xs, xb = x.split(self.expand_chs // 2, dim=1)`\n",
    "    - we pass xb through `block` layers. the number of block layers is defined by depth.\n",
    "    - pass through `conv_transition_b` layer and reduce the feature channels original xb.\n",
    "    - concat xs and xb\n",
    "    - pass through `conv_transition` conv layer to expand to out_chs layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04335250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSPStage(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_chs, \n",
    "                 out_chs, \n",
    "                 stride, \n",
    "                 depth, \n",
    "                 block_ratio=1., \n",
    "                 expand_ratio=1.0, \n",
    "                 bottle_ratio=1.0, \n",
    "                 block_fn=Type[Union[ResNetBlock, ResNetBottleneck]]):\n",
    "        super().__init__()\n",
    "        block_kwargs = {\"norm_layer\":nn.BatchNorm2d, \"act_layer\": nn.ReLU}\n",
    "        self.in_chs = in_chs\n",
    "        self.expand_chs = exp_chs = int(round(out_chs * expand_ratio))\n",
    "        block_out_chs = int(round(out_chs * block_ratio))\n",
    "        conv_kwargs = dict(act_layer=block_kwargs.get('act_layer'), norm_layer=block_kwargs.get('norm_layer'))\n",
    "        aa_layer = block_kwargs.pop('aa_layer', None)\n",
    "        \n",
    "        if stride != 1:\n",
    "            self.conv_down = nn.Sequential(\n",
    "                nn.AvgPool2d(2) if stride == 2 else nn.Identity(),\n",
    "                ConvNormAct(in_chs, out_chs, kernel_size=3, stride=1, **conv_kwargs))\n",
    "            prev_chs = out_chs\n",
    "        else:\n",
    "            self.conv_down = nn.Identity()\n",
    "            prev_chs = in_chs\n",
    "        \n",
    "        self.conv_exp = ConvNormAct(prev_chs, exp_chs, kernel_size=1, apply_act=True, **conv_kwargs)\n",
    "        prev_chs = exp_chs // 2  # output of conv_exp is always split in two\n",
    "        self.in_chs=prev_chs\n",
    "        self.blocks = self._make_layer(block_fn, prev_chs, depth, 1)\n",
    "        # transition convs\n",
    "        self.conv_transition_b = ConvNormAct(self.in_chs, exp_chs // 2, kernel_size=1, **conv_kwargs)\n",
    "        self.conv_transition = ConvNormAct(exp_chs, out_chs, kernel_size=1, **conv_kwargs)\n",
    "        \n",
    "        \n",
    "    def _make_layer(self, block: Type[ResNetBlock], planes: int, blocks: int, stride: int = 1.) -> nn.Sequential:\n",
    "        downsample: Union[nn.Module, partial, None] = None\n",
    "        if stride != 1 or self.in_chs != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                    nn.Conv2d(self.in_chs, planes * block.expansion, kernel_size=1, stride=stride),\n",
    "                    nn.BatchNorm2d(planes * block.expansion),\n",
    "                )\n",
    "        layers = [\n",
    "            block(\n",
    "                in_chs=self.in_chs, out_chs=planes, stride=stride, downsample=downsample\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.in_chs = planes * block.expansion\n",
    "        for _i in range(1, blocks):\n",
    "            layers.append(block(self.in_chs, planes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_down(x)\n",
    "        x = self.conv_exp(x)\n",
    "        xs, xb = x.split(self.expand_chs // 2, dim=1)\n",
    "        xb = self.blocks(xb)\n",
    "        xb = self.conv_transition_b(xb).contiguous()\n",
    "        out = self.conv_transition(torch.cat([xs, xb], dim=1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f32d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2= torch.randn((2,16, 64, 64)).float()\n",
    "tt = CSPStage(16, 64, 2, 2, block_fn=ResNetBottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfb1512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt(x2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a83a32e",
   "metadata": {},
   "source": [
    "### Build CSPResNet and ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78207492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[ResNetBlock, ResNetBottleneck]],\n",
    "        layers: List[int],\n",
    "        block_inplanes: List[int],\n",
    "        in_chs: int = 3,\n",
    "        conv1_t_size: int = 7,\n",
    "        conv1_t_stride: int = 1,\n",
    "        no_max_pool: bool = False,\n",
    "        num_classes: int = 400,\n",
    "        csp: bool=False \n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        # conv-bn-relu\n",
    "        block_kwargs = {\"norm_layer\":nn.BatchNorm2d, \"act_layer\": nn.ReLU}\n",
    "        self.conv1 = ConvNormAct(in_chs, \n",
    "                                self.in_planes, \n",
    "                                kernel_size=conv1_t_size, \n",
    "                                stride=conv1_t_stride, \n",
    "                                padding=tuple(k // 2 for k in (conv1_t_size, conv1_t_size)), \n",
    "                                bias=False, \n",
    "                                **block_kwargs)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        if csp:\n",
    "            self.layer1 = CSPStage(self.in_planes, block_inplanes[0], 1, layers[0], block_fn=block)\n",
    "            self.layer2 = CSPStage(block_inplanes[0], block_inplanes[1], 2, layers[1], block_fn=block)\n",
    "            self.layer3 = CSPStage(block_inplanes[1], block_inplanes[2], 2, layers[2], block_fn=block)\n",
    "            self.layer4 = CSPStage(block_inplanes[2], block_inplanes[3], 2, layers[3], block_fn=block)\n",
    "        else:\n",
    "            \n",
    "            self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],)\n",
    "            self.layer2 = self._make_layer(block, block_inplanes[1], layers[1], stride=2)\n",
    "            self.layer3 = self._make_layer(block, block_inplanes[2], layers[2], stride=2)\n",
    "            self.layer4 = self._make_layer(block, block_inplanes[3], layers[3], stride=2)\n",
    "        #classifier.\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d([1, 1])\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(torch.as_tensor(m.weight), mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(torch.as_tensor(m.weight), 1)\n",
    "                nn.init.constant_(torch.as_tensor(m.bias), 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(torch.as_tensor(m.bias), 0)\n",
    "\n",
    "    def _make_layer(self, block: Type[ResNetBlock], planes: int, blocks: int, stride: int = 1) -> nn.Sequential:\n",
    "        downsample: Union[nn.Module, partial, None] = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                    nn.Conv2d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride),\n",
    "                    nn.BatchNorm2d(planes * block.expansion),\n",
    "                )\n",
    "        layers = [\n",
    "            block(\n",
    "                in_chs=self.in_planes, out_chs=planes, stride=stride, downsample=downsample\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.fc is not None:\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e088aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 128, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 3, 128, 128))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d739b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet10 = ResNet(ResNetBlock, [1, 1, 1, 1], [64, 128, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7636e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 181 ms, sys: 19.7 ms, total: 201 ms\n",
      "Wall time: 213 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 400])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "resnet10(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f08936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csp_resnet10 = ResNet(ResNetBlock, [1, 1, 1, 1], [64, 128, 256, 512], csp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "410317e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 169 ms, sys: 16.8 ms, total: 186 ms\n",
      "Wall time: 186 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 400])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "csp_resnet10(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005277f",
   "metadata": {},
   "source": [
    "As seen on MAC Air M1 the wall time for `csp_resnet` is 186 ms and normal `resnet` is 213 ms, So `csp_resnet` is ~12% faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124f27e",
   "metadata": {},
   "source": [
    "for \n",
    "- resnet18 use `ResNetBlock & [2, 2, 2, 2]`\n",
    "- resnet34 use `ResNetBlock & [3, 4, 6, 3]`\n",
    "- resnet50 use `ResNetBottleneck & [3, 4, 6, 3]`\n",
    "- resnet101 use `ResNetBottleneck & [3, 4, 23, 3]`\n",
    "- resnet200 use `ResNetBottleneck & [3, 24, 36, 3]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c515545",
   "metadata": {},
   "source": [
    "### Parameters count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df75f3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5111888"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res10 = sum([i.numel() for name, i in resnet10.named_parameters()])\n",
    "res10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e241f9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4121616"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cspres10 = sum([i.numel() for name, i in csp_resnet10.named_parameters()])\n",
    "cspres10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60170e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.371942421273705"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((res10- cspres10)/ res10)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5a0f5",
   "metadata": {},
   "source": [
    "> we have ~20% reduction in the number of params. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
